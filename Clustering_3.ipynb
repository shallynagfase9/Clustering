{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4mEup4wvKOcmuANSEvhpk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shallynagfase9/Clustering/blob/main/Clustering_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q1. Explain the basic concept of clustering and give examples of applications where clustering is useful."
      ],
      "metadata": {
        "id": "jbUc1JdDQtJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Clustering is a fundamental technique in unsupervised learning that involves grouping a set of objects (data points) in such a way that objects in the same group (cluster) are more similar to each other than to those in other groups.\n",
        "\n",
        "Examples of Applications Where Clustering is Useful:-\n",
        "\n",
        "- Customer Segmentation:\n",
        "Use Case: Group customers based on purchasing behavior, demographics, or transaction history.\n",
        "Benefits: Tailor marketing strategies, personalize recommendations, optimize product offerings.\n",
        "\n",
        "- Anomaly Detection:\n",
        "Use Case: Identify unusual patterns or outliers in data that do not conform to expected behavior.\n",
        "Benefits: Fraud detection in financial transactions, network intrusion detection, quality control in manufacturing.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "cGx65HiPQvlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q2. What is DBSCAN and how does it differ from other clustering algorithms such as k-means and hierarchical clustering?"
      ],
      "metadata": {
        "id": "KI9OFRpdQvzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "DBSCAN offers advantages in identifying clusters of arbitrary shape, handling noise and outliers effectively, and determining the number of clusters automatically.\n",
        "It is a versatile clustering algorithm suitable for various applications where traditional methods like K-means and hierarchical clustering may not perform optimally.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "NQj7bnJTQyf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q3. How do you determine the optimal values for the epsilon and minimum points parameters in DBSCAN clustering?"
      ],
      "metadata": {
        "id": "_Zz0Ek28Qyn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Determining the optimal values for the epsilon (ϵ) and minimum points (MinPts) parameters in DBSCAN clustering involves selecting values that best capture the underlying structure of your data, specifically focusing on density and distance considerations.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "x9RWRjDMRsp7",
        "outputId": "3a7c824c-8352-4de2-d8b2-c1a2f9f877ee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nDetermining the optimal values for the epsilon (ϵ) and minimum points (MinPts) parameters in DBSCAN clustering involves selecting values that best capture the underlying structure of your data, specifically focusing on density and distance considerations.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q4. How does DBSCAN clustering handle outliers in a dataset?"
      ],
      "metadata": {
        "id": "vVWcUWB-Q1tL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "DBSCAN’s approach to handling outliers revolves around its ability to differentiate between dense regions (clusters) and sparse regions (outliers) based on local density criteria. By using ε and MinPts to define neighborhoods and core points, DBSCAN effectively identifies outliers as points that do not conform to the density expectations of the dataset,\n",
        "making it a valuable tool in clustering tasks where outlier detection is essential.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "9H8X5doZQ3li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q5. How does DBSCAN clustering differ from k-means clustering?"
      ],
      "metadata": {
        "id": "0kaBe1XuQ3v5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "DBSCAN:\n",
        "Suitable for datasets with complex structures or varying densities.\n",
        "- Effective for outlier detection and handling noise in the data.\n",
        "- Used in spatial data analysis, anomaly detection, and clustering applications where the number of clusters is not known in advance.\n",
        "\n",
        "K-means:\n",
        "- Suitable for datasets where clusters are well-separated, spherical, and have roughly equal variance.\n",
        "- Often used in market segmentation, customer analytics, and other applications where the number of clusters is predefined or can be reasonably estimated.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Yi0_i3_tQ6Zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q6. Can DBSCAN clustering be applied to datasets with high dimensional feature spaces? If so, what are some potential challenges?"
      ],
      "metadata": {
        "id": "IAzpQHJXQ6lp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "\n",
        "Yes, DBSCAN (Density-Based Spatial Clustering of Applications with Noise) clustering can be applied to datasets with high-dimensional feature spaces, but there are several potential challenges and considerations to be aware of:\n",
        "\n",
        "Potential Challenges:\n",
        "- Sparse Data: High-dimensional datasets often tend to be sparse, meaning that points are more spread out and may not form dense clusters as expected by DBSCAN.\n",
        "- Noise Sensitivity: DBSCAN may identify noise points more frequently in high-dimensional spaces due to the increased likelihood of points appearing equidistant or having fewer neighbors within the defined ε-radius.\n",
        "- Parameter Sensitivity: The effectiveness of DBSCAN can be highly dependent on the appropriate selection of ε and MinPts parameters. In high-dimensional spaces, finding optimal values that balance noise reduction and cluster detection can be more complex.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "WuivaBxHQ9c5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q7. How does DBSCAN clustering handle clusters with varying densities?"
      ],
      "metadata": {
        "id": "BCXCEQLlQ9pS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "DBSCAN’s ability to handle clusters with varying densities makes it a powerful tool in clustering tasks, particularly in datasets where clusters exhibit different densities and shapes.\n",
        "By leveraging adaptive neighborhood definitions and density-based cluster formation, DBSCAN effectively partitions data points into clusters while robustly identifying noise and outliers.\n",
        "This capability makes DBSCAN suitable for a wide range of applications, including spatial data analysis, anomaly detection, and segmentation tasks where varying density structures are prevalent.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "R9Ina6aLQ_PX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q8. What are some common evaluation metrics used to assess the quality of DBSCAN clustering results?"
      ],
      "metadata": {
        "id": "Gol-JpboQ_X7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Domain-Specific Metrics: Depending on the application, other domain-specific metrics or visual inspections may also be used to evaluate clustering results.\n",
        "\n",
        "Visualization: Visual inspection of clusters and their boundaries can provide qualitative insights into the clustering performance, especially in high-dimensional or complex datasets.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "vOiy2o2gRA_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q9. Can DBSCAN clustering be used for semi-supervised learning tasks?"
      ],
      "metadata": {
        "id": "jhZWRq2aRBKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "DBSCAN itself is not designed explicitly for semi-supervised learning, its output can be leveraged in semi-supervised contexts, particularly for tasks where cluster structure and outlier detection are beneficial.\n",
        "Integrating DBSCAN with semi-supervised learning techniques requires thoughtful parameter tuning, careful cluster interpretation, and consideration of the specific task requirements to effectively utilize clustering results in a semi-supervised learning framework.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "3eq5pOhvRC1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q10. How does DBSCAN clustering handle datasets with noise or missing values?"
      ],
      "metadata": {
        "id": "X9ZUzYnmRDIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) clustering handles datasets with noise or missing values in distinct ways, primarily due to its focus on density-based clustering rather than centroid-based methods.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "V1OH2ROpRE9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Q11. Implement the DBSCAN algorithm using a python programming language, and apply it to a sample dataset. Discuss the clustering results and interpret the meaning of the obtained clusters."
      ],
      "metadata": {
        "id": "oyAPnhGGRFE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "class DBSCAN:\n",
        "    def __init__(self, eps, min_samples):\n",
        "        self.eps = eps  # epsilon: radius of neighborhood\n",
        "        self.min_samples = min_samples  # minimum number of points to form a dense region\n",
        "        self.labels = None  # cluster labels for each point\n",
        "        self.visited = None  # to track visited points\n",
        "\n",
        "    def fit_predict(self, X):\n",
        "        self.labels = np.full(X.shape[0], -1)  # initialize all points as noise (-1)\n",
        "        self.visited = np.zeros(X.shape[0], dtype=bool)  # track visited points\n",
        "\n",
        "        cluster_id = 0  # current cluster ID\n",
        "\n",
        "        for i in range(X.shape[0]):\n",
        "            if not self.visited[i]:\n",
        "                self.visited[i] = True\n",
        "                neighbors = self.region_query(X, i)\n",
        "\n",
        "                if len(neighbors) < self.min_samples:\n",
        "                    self.labels[i] = -1  # mark as noise\n",
        "                else:\n",
        "                    cluster_id += 1\n",
        "                    self.expand_cluster(X, i, neighbors, cluster_id)\n",
        "\n",
        "        return self.labels\n",
        "\n",
        "    def region_query(self, X, i):\n",
        "        \"\"\"Find all points within epsilon neighborhood of point X[i]\"\"\"\n",
        "        neighbors = []\n",
        "        for j in range(X.shape[0]):\n",
        "            if np.linalg.norm(X[i] - X[j]) <= self.eps:\n",
        "                neighbors.append(j)\n",
        "        return neighbors\n",
        "\n",
        "    def expand_cluster(self, X, i, neighbors, cluster_id):\n",
        "        \"\"\"Expand cluster from point X[i]\"\"\"\n",
        "        self.labels[i] = cluster_id\n",
        "\n",
        "        for neighbor in neighbors:\n",
        "            if not self.visited[neighbor]:\n",
        "                self.visited[neighbor] = True\n",
        "                new_neighbors = self.region_query(X, neighbor)\n",
        "\n",
        "                if len(new_neighbors) >= self.min_samples:\n",
        "                    neighbors.extend(new_neighbors)\n",
        "\n",
        "            if self.labels[neighbor] == -1:\n",
        "                self.labels[neighbor] = cluster_id\n",
        "\n",
        "# Example usage with a sample dataset\n",
        "if __name__ == \"__main__\":\n",
        "    # Sample dataset\n",
        "    X = np.array([[1, 2], [2, 2], [2, 3],\n",
        "                  [8, 7], [8, 8], [25, 80]])\n",
        "\n",
        "    # Initialize and fit DBSCAN\n",
        "    dbscan = DBSCAN(eps=3, min_samples=2)\n",
        "    labels = dbscan.fit_predict(X)\n",
        "\n",
        "    # Print cluster labels and points\n",
        "    print(\"Cluster labels:\", labels)\n",
        "\n",
        "    # Interpretation of clusters\n",
        "    unique_labels = np.unique(labels)\n",
        "    n_clusters = len(unique_labels) - (1 if -1 in labels else 0)  # number of clusters\n",
        "\n",
        "    for cluster_id in range(n_clusters):\n",
        "        cluster_points = X[labels == cluster_id]\n",
        "        print(f\"Cluster {cluster_id}:\")\n",
        "        print(cluster_points)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4z9O6--tRIR7",
        "outputId": "35504ecb-43f5-45c3-afd0-96c9d9bc8450"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster labels: [ 1  1  1  2  2 -1]\n",
            "Cluster 0:\n",
            "[]\n",
            "Cluster 1:\n",
            "[[1 2]\n",
            " [2 2]\n",
            " [2 3]]\n"
          ]
        }
      ]
    }
  ]
}